{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [' https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies3.html',\n",
    "        'https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies2.html',\n",
    "        'https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies1.html']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract urls and save :Article_i.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "movie_links = []\n",
    "for url in urls:\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  for link in soup.find_all('a', href = True):\n",
    "    movie_links.append(link['href'])\n",
    "for movie in range (30000+1):\n",
    "  response_page = requests.get(movie_links[movie])\n",
    "  if int(response_page.status_code)==200:\n",
    "    time.sleep(random.randint(1,5))\n",
    "  elif  int(response_page.status_code)==404: # Not Found\n",
    "    print ('url not found')\n",
    "    time.sleep(random.randint(1,5))\n",
    "  elif int(response_page.status_code)!=429: #Too Many Requests\n",
    "    time.sleep(1500)\n",
    "  soup = BeautifulSoup(response_page.text, 'html.parser')\n",
    "  name = \"Article_{0}.html\".format(movie)\n",
    "  with open (name, 'w') as file:\n",
    "    file.write(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract data include name intro plot and etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i try with one link for test code \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import lxml.html\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "response = requests.get('https://en.wikipedia.org/wiki/10_to_Midnight') \n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# find intro location and extract text\n",
    "intro = soup.select_one('div#bodyContent div#mw-content-text p').text\n",
    "\n",
    "# find plot location and extract text\n",
    "plot = soup.select_one('div#bodyContent div#mw-content-text').find_all('p')[1].text\n",
    "\n",
    "# remove stop words intro\n",
    "stop_words = set(stopwords.words('english')) \n",
    "text_tokens_intro = word_tokenize(intro)\n",
    "tokens_without_sw_intro = [word for word in text_tokens_intro if not word in stopwords.words()]\n",
    "\n",
    "# remove stop words plot\n",
    "text_tokens_plot = word_tokenize(plot)\n",
    "tokens_without_sw_plot = [word for word in text_tokens_plot if not word in stopwords.words()]\n",
    "\n",
    "# remove punctuation intro \n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "punc_intro = \",\".join(tokens_without_sw_intro)\n",
    "new_words_intro = tokenizer.tokenize(punc_intro)\n",
    "\n",
    "# remove punctuation plot \n",
    "punc_plot = \",\".join(tokens_without_sw_plot)\n",
    "new_words_plot = tokenizer.tokenize(punc_plot)\n",
    "\n",
    "\n",
    "#Stemming intro and plot \n",
    "\n",
    "ps = PorterStemmer()\n",
    "streaming_intro = []\n",
    "streaming_plot = []\n",
    "for x in new_words_intro:\n",
    "    streaming_intro.append(ps.stem(x))\n",
    "for y in new_words_plot:\n",
    "    streaming_plot.append(ps.stem(y))\n",
    "\n",
    "\n",
    "table = soup.find('table', class_ = 'infobox vevent')\n",
    "element_tree = lxml.html.fromstring(response.text)\n",
    "check_list = ['movie name','Directed by','Produced by','Written by','Starring','Music by','Release date','Running time','Country','Language','Budget']\n",
    "info = []\n",
    "stars ='//*[@id=\"mw-content-text\"]/div[1]/table/tbody/tr[6]/td/div/ul/li/*/text()'\n",
    "writen ='//*[@id=\"mw-content-text\"]/div[1]/table/tbody/tr[5]/td//a/text()'\n",
    "Path =[\n",
    "    '//*[@id=\"mw-content-text\"]/div[1]/table/tbody/tr[1]/th/text()',\n",
    "    '//*[@id=\"mw-content-text\"]/div[1]/table/tbody/tr[3]/td/a/text()',\n",
    "    '//*[@id=\"mw-content-text\"]/div[1]/table/tbody/tr[4]/td/text()',\n",
    "    writen,\n",
    "    stars,\n",
    "    '//*[@id=\"mw-content-text\"]/div[1]/table/tbody/tr[7]/td/a/text()',\n",
    "    '//*[@id=\"mw-content-text\"]/div[1]/table/tbody/tr[12]/td/div/ul/li/text()',\n",
    "    '//*[@id=\"mw-content-text\"]/div[1]/table/tbody/tr[13]/td/text()',\n",
    "    '//*[@id=\"mw-content-text\"]/div[1]/table/tbody/tr[14]/td/text()',\n",
    "    '//*[@id=\"mw-content-text\"]/div[1]/table/tbody/tr[15]/td/text()',\n",
    "    '//*[@id=\"mw-content-text\"]/div[1]/table/tbody/tr[16]/td/text()'\n",
    "  \n",
    "]\n",
    "for i in Path:\n",
    "    if element_tree.xpath(i):\n",
    "        info.append(element_tree.xpath(i))\n",
    "    else:\n",
    "        info.append(\"NA\")\n",
    "  \n",
    "\n",
    "\n",
    "print(\"*\"*100)\n",
    "data = []\n",
    "# data inclue intro plot and check_list\n",
    "data.append(streaming_intro)\n",
    "data.append(streaming_plot)\n",
    "for i in info:\n",
    "    if len(i)==1:\n",
    "        data.append(*(i))\n",
    "    elif len(i) > 1:\n",
    "        data.append(i)\n",
    "      \n",
    "#     elif len(i) > 1:\n",
    "#         for x in i:\n",
    "#             new_info.append(x)      \n",
    "  \n",
    "with open('output.tsv', 'w', newline='') as f_output:\n",
    "    tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "    tsv_output.writerow(data)\n",
    "     \n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
